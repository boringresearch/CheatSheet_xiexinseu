# 概率论基础

## 概率与分布

### 条件概率与独立事件
1. 条件概率：已知 $A$ 事件发生的条件下 $B$ 发生的概率，记作 $P(B|A)$，它等于事件 $AB$ 的概率相对于事件 $A$ 的概率，即：
$$
P(B|A)=\frac{P(AB)}{P(A)}
$$
其中必须有$P(A)>0$
2. 条件概率分布的链式法则：对于 $n$ 个随机变量 $\mathbf{x_{1},x_{2},\cdots,x_{n}}$，有：
$$
P(\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{n})=
P(\mathbf{x}_{1})\prod_{i=2}^{n}P(\mathbf{x}_{i}|\mathbf{x}_{1},\cdots,\mathbf{x}_{i-1})
$$
3. 两个随机变量 $\mathbf{x,y}$ 相互独立的数学描述：
$$
\forall x\in \mathcal{X},\forall y\in \mathcal{Y},P(\mathbf{x}=x,\mathbf{y}=y)=P(\mathbf{x}=x)P(\mathbf{y}=y)
$$
记作： $\mathbf{x}\bot\mathbf{y}$
4. 两个随机变量 $\mathbf{x,y}$ 关于随机变量 $z$ 条件独立的数学描述：
$$
\begin{aligned}
    & \forall x\in\mathcal{X},\forall y\in\mathcal{Y},\forall z\in\mathcal{Z} \\
    & P(\mathbf{x}=x,\mathbf{y}=y,\mathbf{z}=z)=P(\mathbf{x}=x|\mathbf{z}=z)P(\mathbf{y}=y|\mathbf{z}=z)
\end{aligned}
$$
记作： $(\mathbf{x}\bot\mathbf{y})|\mathbf{z}$
### 联合概率分布
1. 定义 $\mathbf{x}$ 和 $\mathbf{y}$ 的联合分布为：
$$
P(a,b)=P\{\mathbf{x}\le a,\mathbf{y}\le b \},-\infty <a,b <+\infty
$$
2. $\mathbf{x}$ 的分布可以从联合分布中得到：
$$
P_{x}(a)=P\{\mathbf{x}\le a\}=
P\{\mathbf{x}\le a,\mathbf{y}\le\infty\}=P(a,\infty),-\infty<a<+\infty
$$
类似的，$\mathbf{y}$ 的分布可以从联合分布中得到：
$$
P_{y}(b)=P\{\mathbf{y}\le b\}=
P\{\mathbf{x}\le \infty,\mathbf{y}\le b\}=P(\infty,b),-\infty<b<+\infty
$$
3. 当 $\mathbf{x}$ 和 $\mathbf{y}$ 都是离散随机变量时， 定义 $\mathbf{x}$ 和 $\mathbf{y}$ 的联合概率质量函数为： $p(x,y)=P\{ \mathbf{x}=x,\mathbf{y}=y\}$
则 $\mathbf{x}$ 和 $\mathbf{y}$ 的概率质量函数分布为：
$$
p_{\mathbf{x}}(x)=\sum_{y:p(x,y)>0}p(x,y),
p_{\mathbf{y}}(y)=\sum_{x:p(x,y)>0}p(x,y),
$$
4. 当 $\mathbf{x}$ 和 $\mathbf{y}$ 联合地连续时，即存在函数 $p(x,y)$ ，使得对于所有的实数集合 $A$ 和 $B$ 满足：
$$
P\{ \mathbf{x}\in A,\mathbf{y}\in B \}=
\int_{B}\int_{A}p(x,y)dxdy
$$
则函数 $p(x,y)$ 称为 $\mathbf{x}$ 和 $\mathbf{y}$ 的概率密度函数。
* 联合分布为
$$
P(a,b)=P\{ \mathbf{x}\le a,\mathbf{y}\le begin \}=
\int_{-\infty}^{a}\int_{-\infty}^{b}p(x,y)dxdy
$$
* $\mathbf{x}$和 $\mathbf{y}$ 的概率密度函数以及分布函数分别为：
$$
\begin{aligned}
    & P_{\mathbf{x}}(a)=\int_{-\infty}^{a}\int_{-\infty}^{\infty}p(x,y)dxdy=\int_{-\infty}^{a}p_{\mathbf{x}}(x)dx \\
    & P_{\mathbf{y}}(b)=\int_{-\infty}^{\infty}\int_{-\infty}^{b}p(x,y)dxdy=\int_{-\infty}^{b}p_{\mathbf{y}}(y)dy \\
    & p_{\mathbf{x}}(x)=\int_{-\infty}^{\infty}p(x,y)dy \\
    & p_{\mathbf{y}}(y)=\int_{-\infty}^{\infty}p(x,y)dx
\end{aligned}
$$

## 期望
1. 期望：（是概率分布的泛函，函数的函数）
* 离散型随机变量 $\mathbf{x}$ 的期望：
$$
\mathbb{E}[x]=\sum_{i=1}^{\infty}x_{i}p_{i}
$$
若级数不收敛，则期望不存在
* 连续性随机变量 $\mathbf{x}$ 的期望：
$$
\mathbb{E}[x]=\int_{-\infty}^{\infty}xp(x)dx
$$
若极限不收敛，则期望不存在
2. 期望描述了随机变量的平均情况，衡量了随机变量 $\mathbf{x}$ 的均值
3. 定理：设 $\mathbf{y}=g(\mathbf{x})$ 均为随机变量，$g(\cdot)$ 是连续函数
* 若 $\mathbf{x}$ 为离散型随机变量，若 $\mathbf{y}$ 的期望存在，则：
$$
\mathbb{E}[\mathbf{y}]=\mathbb{E}[g(\mathbf{x})]=\sum_{i=1}^{\infty}g(x_{i})p_{i}
$$
* 若 $\mathbf{x}$ 为连续型随机变量，若 $\mathbf{y}$ 的期望存在，则 ：
$$
\mathbb{E}[\mathbf{y}]=\mathbb{E}[g(\mathbf{x})]=\int_{-\infty}^{\infty}g(x)p(x)dx
$$
该定理的意义在于：当求 $\mathbb{E}(\mathbf{y})$ 时，不必计算出 $\mathbf{y}$ 的分布，只需要利用 $\mathbf{x}$ 的分布即可。该定理可以推广至两个或者两个以上随机变量的情况。此时：
$$
\mathbb{E}[\mathbf{z}]=\mathbb{E}[g(\mathbf{x,y})]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}g(x,y)p(x,y)dxdy
$$
>上述公式也记做：
$$
\begin{aligned}
    & \mathbb{E}_{\mathbf{x}\sim P}[g(x)]=\sum_{x}g(x)p(x) \\
    & \mathbb{E}_{\mathbf{x}\sim P}[g(x)]=\int_{x}g(x)p(x)dx \\
    & \mathbb{E}_{\mathbf{x},\mathbf{y}\sim P}[g(x,y)]=\int_{x}g(x,y)p(x,y)dxdy \\
\end{aligned}
$$
4. 期望性质：
* 常数的期望就是常数本身
* 对常数 $C$ 有 ：
$$
\mathbb{E}[C\mathbf{x}]=C\mathbb{E}[\mathbf{x}]
$$
* 对两个随机变量 $\mathbf{x,y}$ ，有：
$$
\mathbb{E}[\mathbf{x+y}]=\mathbb{E}[\mathbf{x}]+\mathbb{E}[\mathbf{y}]
$$
该结论可以推广到任意有限个随机变量之和的情况
* 对两个相互独立的随机变量，有：
$$
\mathbb{E}[\mathbf{xy}]=\mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{y}]
$$
该结论可以推广到任意有限个相互独立的随机变量之积的情况
## 方差

### 方差
1. 对随机变量 $\mathbf{x}$ ，若 $\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{2}]$ 存在，则称它为 $\mathbf{x}$ 的方差，记作 $\mathrm{Var}[\mathbf{x}]$ 。 $\mathbf{x}$ 的标准差为方差的开平方。即：
$$
\begin{aligned}
    & \mathrm{Var}[\mathbf{x}]=\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{2}] \\
    & \sigma=\sqrt{\mathrm{Var}[\mathbf{x}]}
\end{aligned}
$$
* 方差度量了随机变量 $\mathbf{x}$ 与期望值偏离的程度，衡量了 $\mathbf{x}$  取值分散程度的一个尺度。
* 由于绝对值 $|\mathbf{x}-\mathbb{E}[\mathbf{x}]|$ 带有绝对值，不方便运算，因此采用平方来计算。又因为 $|\mathbf{x}-\mathbb{E}[\mathbf{x}]|^{2}$ 是一个随机变量，因此对它取期望，即得 $\mathbf{x}$ 与期望值偏离的均值
2. 根据定义可知：
$$
\begin{aligned}
    & \mathrm{Var}[\mathbf{x}]=\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{2}]=\mathbb{E}[\mathbb{x}^{2}]-(\mathbb{E}[\mathbf{x}])^{2} \\
    & \mathrm{Var}[f(\mathbf{x})]=\mathbb{E}[(f(\mathbf{x})-\mathbb{E}[f(\mathbf{x})])^{2}]
\end{aligned}
$$
3. 对于一个期望为 $\mu$， 方差为 $\sigma^{2},\sigma\neq0$ 的随机变量 $\mathbf{x}$，随机变量 $\mathbf{x}^{*}=\frac{\mathbf{x}-\mu}{\sigma}$ 的数学期望为0，方差为1。 称 $\mathbf{x}^{*}$ 为 $\mathbf{x}$ 的标准化变量
4. 方差的性质：
* 常数的方差恒为0
* 对常数 $C$ 有 $\mathrm{Var}[C\mathbf{x}]=C^{2}\mathrm{Var}[\mathbf{x}]$
* 对两个随机变量 $\mathbf{x,y}$，有： $\mathrm{Var}[\mathbf{x+y}]=\mathrm{Var}[\mathbf{x}]+\mathrm{Var}[\mathbf{y}]+2\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])(\mathbf{y}-\mathbb{E}[\mathbf{y}])]$
    * 当 $\mathbf{x}$  和 $\mathbf{y}$ 相互独立时，有 $\mathrm{Var}[\mathbf{x+y}]=\mathrm{Var}[\mathbf{x}]+\mathrm{Var}[\mathbf{y}]$ 。可以推广至任意有限多个相互独立的随机变量之和的情况
* $\mathrm{Var}[\mathbf{x}]=0$ 的充要条件是 $\mathbf{x}$ 以概率1取常数
### 协方差与相关系数
1. 对于二维随机变量 $(\mathbf{x,y})$ ，可以讨论描述 $\mathbf{x}$ 与 $\mathbf{y}$ 之间相互关系的数字特征。
* 定义  $\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])(\mathbf{y}-\mathbb{E}[\mathbf{y}])]$ 为随机变量 $\mathbf{x}$ 与 $\mathbf{y}$ 的协方差，记作 $\mathrm{Cov}[\mathbf{x,y}]=\mathbb{E}[(\mathbf{x}-\mathbb{E}[\mathbf{x}])(\mathbf{y}-\mathbb{E}[\mathbf{y}])]$  。
* 定义 $\rho_{\mathbf{x,y}}=\frac{\mathrm{Cov[\mathbf{x,y}]}}{\sqrt{\mathrm{Var}[\mathbf{x}]\mathrm{Var}[\mathbf{y}]}}$  为随机变量 $\mathbf{x}$ 与　$\mathbf{y}$　的相关系数，它是协方差的归一化。
2. 由定义可知：
$$
\begin{aligned}
    & \mathrm{Cov}[\mathbf{x,y}]=\mathrm{Cov}[\mathbf{y,x}] \\
    & \mathrm{Cov}[\mathbf{x,x}]=\mathrm{Var}[\mathbf{x}] \\
    & \mathrm{Var}[\mathbf{x+y}]=\mathrm{Var}[\mathbf{x}]+\mathrm{Var}[\mathbf{y}]+2\mathrm{Cov}[\mathbf{x,y}]
\end{aligned}
$$
3. 协方差的性质：
* $\mathrm{Cov}[a\mathbf{x},b\mathbf{y}]=ab\mathrm{Cov}[\mathbf{x,y}],a,b$为常数
* $\mathrm{Cov}[\mathbf{x}_{1}+\mathbf{x}_{2},\mathbf{y}]=\mathrm{Cov}[\mathbf{x}_{1},\mathbf{y}]+\mathrm{Cov}[\mathbf{x}_{2},\mathbf{y}]$
* $\mathrm{Cov}[f(\mathbf{x}),g(\mathbf{y})]=\frac{\mathrm{Cov}[f(\mathbf{x}),g(\mathbf{y})]}{\sqrt{\mathrm{Var}[f(\mathbf{x})]\mathrm{Var}[f(\mathbf{y})]}}$
4. 协方差的物理意义：
* 协方差的绝对值越大，说明两个随机变量都远离它们的均值。
* 协方差如果为正，则说明两个随机变量同时趋向于取较大的值；如果为负，则说明一个随变量趋向于取较大的值，另一个随机变量趋向于取较小的值
* 两个随机变量的独立性可以导出协方差为零。但是两个随机变量的协方差为零无法导出独立性
    * 因为独立性也包括：没有非线性关系。有可能两个随机变量是非独立的，但是协方差为零
    * 假设随机变量 $\mathbf{x}\sim U[-1,1]$ 。定义随机变量 $\mathbf{s}$ 的概率分布函数为：$P(\mathbf{s}=1)=\frac{1}{2}P(\mathbf{s}=-1)=\frac{1}{2}$,定义随机变量 $\mathbf{y=sx}$，则随机变量 $\mathbf{x,y}$ 是非独立的，但是有： $\mathrm{Cov}[\mathbf{x,y}]=0$
5. 相关系数的物理意义：考虑以随机变量  的线性函数  来近似表示 。以均方误差



